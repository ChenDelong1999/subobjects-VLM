{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.visualization import visualize_sample\n",
    "from model.utils import create_vlm\n",
    "from model.utils import VisualTextualTokenization\n",
    "from data import get_dataset\n",
    "from visual_tokenizer import get_visual_tokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data import (\n",
    "    ShareGPT4V,\n",
    "    ImageNet,\n",
    "    Cambrian,\n",
    "    CLEVRCaption,\n",
    "    PixmoDataset\n",
    ")\n",
    "\n",
    "dataset = PixmoDataset(root='/private/home/delong/workspace/data/pixmo-cap', split='val')\n",
    "\n",
    "# dataset = CLEVRCaption(root='/private/home/delong/workspace/data/clevr-caption', split='val')\n",
    "\n",
    "# dataset = ImageNet(root='/datasets01/imagenet_full_size/061417', split='train')\n",
    "\n",
    "# dataset = ShareGPT4V(\n",
    "#     root='/private/home/delong/workspace/data/ShareGPT4V',\n",
    "#     split='sharegpt4v_instruct_gpt4-vision_cap100k.json')\n",
    "\n",
    "\n",
    "# dataset = ShareGPT4V(\n",
    "#     root='/private/home/delong/workspace/data/ShareGPT4V',\n",
    "#     split='share-captioner_coco_lcs_sam_1246k_1107.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/private/home/delong/workspace/subobjects-VLM/runs/pixmo_cap/Llama-3_2-1B-dinov2_small(768px)/superpixel/1224-1559-superpixel_slic(100t-768px)/runs/checkpoint-8283\"\n",
    "model, textual_tokenizer = create_vlm(\n",
    "    checkpoint, \n",
    "    llm_class='smollm'\n",
    "    )\n",
    "\n",
    "model = model.cuda().half().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resolution = 768\n",
    "max_tokens = 100\n",
    "\n",
    "# config = json.load(open('configs/visual_tokenizer/directsam/directsam_tiny_sa1b_2ep@0.05.json'))\n",
    "# config = json.load(open('configs/visual_tokenizer/patch/patch_16_per_side_random.json'))\n",
    "config = json.load(open('configs/visual_tokenizer/superpixel/superpixel_slic.json'))\n",
    "\n",
    "visual_tokenizer = get_visual_tokenizer(**config, image_resolution=image_resolution, max_tokens=max_tokens)\n",
    "\n",
    "vl_tokenizer = VisualTextualTokenization(textual_tokenizer, visual_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "loss = 0\n",
    "for _ in tqdm.tqdm(range(n_samples)):\n",
    "    sample = dataset[random.randint(0, len(dataset))]\n",
    "    inputs = vl_tokenizer([sample], eval=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss += outputs['loss'].item()\n",
    "\n",
    "print(f\"Loss: {loss / n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "# sample = dataset[random.randint(0, len(dataset))]\n",
    "\n",
    "print(sample['text'])\n",
    "\n",
    "label = sample['text'].split('<|assistant|>')[1].strip().replace(textual_tokenizer.eos_token, '')\n",
    "sample['text'] = sample['text'].split('<|assistant|>')[0] + '<|assistant|>'\n",
    "\n",
    "inputs = vl_tokenizer([sample], eval=True)\n",
    "\n",
    "\n",
    "inputs_embeds, labels = model.prepare_inputs_embeds(\n",
    "    inputs['text'], inputs['image'], inputs['masks']\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=dataset.max_text_tokens,\n",
    "    eos_token_id = textual_tokenizer.eos_token_id,\n",
    "    pad_token_id = textual_tokenizer.pad_token_id,\n",
    ")\n",
    "prediction = textual_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "visualize_sample(sample, inputs)\n",
    "print(label)\n",
    "print('-' * 80)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
