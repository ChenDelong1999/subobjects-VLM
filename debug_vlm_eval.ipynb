{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.visualization import visualize_sample\n",
    "from visual_tokenizer.directsam import DirectSAMTokenizer\n",
    "from model.utils import create_vlm\n",
    "from model.utils import VisualTextualTokenization\n",
    "from data import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = get_dataset('coco', '/share/datasets/coco2017', split='train')\n",
    "# eval_dataset = get_dataset('clevr_caption', '/home/dchenbs/workspace/datasets/CLEVR_v1.0', split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"runs/0919-1632-coco-vae-no-query-SmolLM-360M-Instruct/checkpoint-2000\"\n",
    "model, textual_tokenizer = create_vlm(checkpoint)\n",
    "\n",
    "model = model.cuda().half().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_tokenizer = DirectSAMTokenizer(\n",
    "    checkpoint=\"chendelong/DirectSAM-tiny-distilled-30ep-plus-50ep-1024px-0910\",\n",
    "    threshold=0.1,\n",
    "    image_resolution=model.config.vlm_config.image_resolution,\n",
    "    max_tokens=128,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "vl_tokenizer = VisualTextualTokenization(textual_tokenizer, visual_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "loss = 0\n",
    "for _ in tqdm.tqdm(range(n_samples)):\n",
    "    sample = eval_dataset[random.randint(0, len(eval_dataset))]\n",
    "    inputs = vl_tokenizer([sample], eval=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss += outputs['loss'].item()\n",
    "\n",
    "print(f\"Loss: {loss / n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = eval_dataset[random.randint(0, len(eval_dataset))]\n",
    "\n",
    "label = sample['text'].split('<|assistant|>')[1].strip().replace(textual_tokenizer.eos_token, '')\n",
    "sample['text'] = sample['text'].split('<|assistant|>')[0] + '<|assistant|>'\n",
    "\n",
    "inputs = vl_tokenizer([sample], eval=True)\n",
    "\n",
    "\n",
    "inputs_embeds, labels = model.prepare_inputs_embeds(\n",
    "    inputs['text'], inputs['image'], inputs['masks']\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=eval_dataset.max_text_tokens,\n",
    "    eos_token_id = textual_tokenizer.eos_token_id,\n",
    "    pad_token_id = textual_tokenizer.pad_token_id,\n",
    ")\n",
    "prediction = textual_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "visualize_sample(sample, inputs)\n",
    "print(label)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
