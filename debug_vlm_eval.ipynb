{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.visualization import visualize_sample\n",
    "from model.utils import create_vlm\n",
    "from model.utils import VisualTextualTokenization\n",
    "from data import get_dataset\n",
    "from visual_tokenizer import get_visual_tokenizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_dataset('imagenet', '/share/datasets/imagenet', split='val')\n",
    "# dataset = get_dataset('coco', '/share/datasets/coco2017', split='val')\n",
    "dataset = get_dataset('clevr_caption', '/home/dchenbs/workspace/datasets/CLEVR_v1.0', split='val')\n",
    "# dataset = get_dataset('image_paragraph_captioning', '/home/dchenbs/workspace/datasets/VisualGenome', split='val')\n",
    "\n",
    "\n",
    "# dataset = get_dataset('sharegpt4v', '/home/dchenbs/workspace/datasets/sharegpt4v/ShareGPT4V/sharegpt4v_instruct_gpt4-vision_cap100k.json', split='val')\n",
    "\n",
    "# dataset = ShareGPT4V('/home/dchenbs/workspace/datasets/sharegpt4v/ShareGPT4V/sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json', split=split)\n",
    "# dataset = ShareGPT4V('/home/dchenbs/workspace/datasets/sharegpt4v/ShareGPT4V/share-captioner_coco_lcs_sam_1246k_1107.json', split=split)\n",
    "# dataset = ShareGPT4V('/home/dchenbs/workspace/datasets/sharegpt4v/ShareGPT4V/sharegpt4v_instruct_gpt4-vision_cap100k.json', split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/home/dchenbs/workspace/subobjects-VLM/runs/0922-2018-clevr_caption-directsam_tiny(36)-rgb_pixel-SmolLM-1_7B-Instruct/checkpoint-2187\"\n",
    "model, textual_tokenizer = create_vlm(checkpoint)\n",
    "\n",
    "model = model.cuda().half().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resolution = 768\n",
    "max_tokens = 36\n",
    "\n",
    "# config = json.load(open('configs/visual_tokenizer/patch_6_per_side_random.json'))\n",
    "# config = json.load(open('configs/visual_tokenizer/patch_8_per_side_raster.json'))\n",
    "# config = json.load(open('configs/visual_tokenizer/directsam_0424.json'))\n",
    "config = json.load(open('configs/visual_tokenizer/directsam_tiny.json'))\n",
    "\n",
    "visual_tokenizer = get_visual_tokenizer(**config, image_resolution=image_resolution, max_tokens=max_tokens)\n",
    "\n",
    "vl_tokenizer = VisualTextualTokenization(textual_tokenizer, visual_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "loss = 0\n",
    "for _ in tqdm.tqdm(range(n_samples)):\n",
    "    sample = dataset[random.randint(0, len(dataset))]\n",
    "    inputs = vl_tokenizer([sample], eval=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss += outputs['loss'].item()\n",
    "\n",
    "print(f\"Loss: {loss / n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[random.randint(0, len(dataset))]\n",
    "\n",
    "label = sample['text'].split('<|assistant|>')[1].strip().replace(textual_tokenizer.eos_token, '')\n",
    "sample['text'] = sample['text'].split('<|assistant|>')[0] + '<|assistant|>'\n",
    "\n",
    "inputs = vl_tokenizer([sample], eval=True)\n",
    "\n",
    "\n",
    "inputs_embeds, labels = model.prepare_inputs_embeds(\n",
    "    inputs['text'], inputs['image'], inputs['masks']\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=dataset.max_text_tokens,\n",
    "    eos_token_id = textual_tokenizer.eos_token_id,\n",
    "    pad_token_id = textual_tokenizer.pad_token_id,\n",
    ")\n",
    "prediction = textual_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "visualize_sample(sample, inputs)\n",
    "print(label)\n",
    "print('-' * 80)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects_vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
