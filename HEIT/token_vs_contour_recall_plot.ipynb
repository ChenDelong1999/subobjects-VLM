{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_dir = 'outputs/token_vs_contour_recall'\n",
    "# output_dir = 'outputs/token_vs_contour_recall_directsam_threshold_ablation@448'\n",
    "\n",
    "\n",
    "all_tokenizers = []\n",
    "summary = {}\n",
    "splits = os.listdir(output_dir)\n",
    "for split in splits:\n",
    "    split_dir = os.path.join(output_dir, split)\n",
    "    summary[split] = {}\n",
    "    files = os.listdir(split_dir)\n",
    "    files.sort()\n",
    "    for f in files:\n",
    "        if f.endswith('.json'):\n",
    "            results = json.load(open(os.path.join(split_dir, f)))\n",
    "            tokenizer = f.split('_202')[0]\n",
    "            all_tokenizers.append(tokenizer)\n",
    "            summary[split][tokenizer]= [results['mean_tokens'], results['mean_recall']]\n",
    "\n",
    "print(summary)\n",
    "all_tokenizers = list(set(all_tokenizers))\n",
    "all_tokenizers.sort()\n",
    "all_invalid_tokenizers = []\n",
    "for tokenizer in all_tokenizers:\n",
    "    print(tokenizer)\n",
    "    # check missing splits for this tokenizer\n",
    "    for split in splits:\n",
    "    # for split in ['SA1B']:\n",
    "        if tokenizer not in summary[split]:\n",
    "            print(f\" - Missing {split}\")\n",
    "            all_invalid_tokenizers.append(tokenizer)\n",
    "\n",
    "all_invalid_tokenizers = list(set(all_invalid_tokenizers))\n",
    "# remove invalid tokenizers\n",
    "for tokenizer in all_invalid_tokenizers:\n",
    "    all_tokenizers.remove(tokenizer)\n",
    "\n",
    "ignore_tokenizers = [\n",
    "    'directsam_large_gen1_1008',\n",
    "    'directsam_large_gen2_1014',\n",
    "    'directsam_large_gen3_1023',\n",
    "    'directsam_large_sa1b_2ep',\n",
    "    'directsam_tiny_dsa_50ep',\n",
    "    # 'directsam_tiny_dsa_75ep',\n",
    "    'directsam_tiny_sa1b_2ep',\n",
    "\n",
    "    'panoptic_mask2former_base',\n",
    "    'panoptic_mask2former_small',\n",
    "    'panoptic_mask2former_tiny',\n",
    "\n",
    "    'panoptic_oneformer_tiny',\n",
    "\n",
    "]\n",
    "for tokenizer in ignore_tokenizers:\n",
    "    all_tokenizers.remove(tokenizer) if tokenizer in all_tokenizers else None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LEVELS = [\n",
    "    'tcd', 'PhenoBench', 'EgoHOS', 'LoveDA', \n",
    "    'DUTS_TE', 'LVIS', \n",
    "    'COCONut_relabeld_COCO_val', \n",
    "    'FoodSeg103', 'plantorgans', 'SUIM', \n",
    "    'LIP', \n",
    "    'MyFood', 'DIS5K_DIS_VD', 'WireFrame', 'EntitySeg', 'Fashionpedia', 'SOBA', \n",
    "    'ISAID', 'MapillaryMetropolis', 'PACO', \n",
    "    'PascalPanopticParts', \n",
    "    'ADE20k', \n",
    "    'SeginW', \n",
    "    'CIHP', 'NYUDepthv2', 'cityscapes', 'DRAM', \n",
    "    'PartImageNetPP', \n",
    "    'SPIN'\n",
    "]\n",
    "\n",
    "OBJECT_LEVEL = [\n",
    "    \"EgoHOS\", \"DUTS_TE\", \"LVIS\",\n",
    "    \"FoodSeg103\", \"SUIM\", \"MyFood\", \"DIS5K_DIS_VD\", \"EntitySeg\", \"NYUDepthv2\", \"cityscapes\", \"DRAM\"\n",
    "]\n",
    "\n",
    "\n",
    "SUBOBJECT_LEVEL = [\n",
    "    \"LIP\", \"PACO\", \"PascalPanopticParts\", \"ADE20k\", \"CIHP\", \"PartImageNetPP\", \"SPIN\", \"Fashionpedia\"\n",
    "]\n",
    "\n",
    "classification = {\n",
    "    # 'All': ALL_LEVELS,\n",
    "    'Object': OBJECT_LEVEL,\n",
    "    'Subobject': SUBOBJECT_LEVEL,\n",
    "    # 'SA1B': ['SA1B'],\n",
    "\n",
    "}\n",
    "\n",
    "for name, SPLITS in classification.items():\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    for tokenizer in all_tokenizers:\n",
    "        avg_tokens = 0\n",
    "        avg_recall = 0\n",
    "        for split in SPLITS:\n",
    "            if tokenizer not in summary[split]:\n",
    "                raise ValueError(f\"Missing {tokenizer} for {split}\")\n",
    "            tokens, recall = summary[split][tokenizer]\n",
    "            avg_tokens += tokens\n",
    "            avg_recall += recall\n",
    "        avg_tokens /= len(SPLITS)\n",
    "        avg_recall /= len(SPLITS)\n",
    "\n",
    "        plt.scatter(avg_tokens, avg_recall, label=tokenizer)\n",
    "        plt.text(avg_tokens, avg_recall, f\"  {tokenizer}\\n   - {avg_recall*100:.1f}% recall\\n   - {int(avg_tokens)} tokens\", fontsize=9)\n",
    "\n",
    "\n",
    "    plt.xlabel('Average Tokens')\n",
    "    plt.ylabel('Average Recall')\n",
    "    # plt.title(f'Token vs Contour Recall ({SPLITS})')\n",
    "    # instead use var name of SPLITS\n",
    "    plt.title(f'Token vs Contour Recall ({name})')\n",
    "    plt.xlim(0, 500)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
