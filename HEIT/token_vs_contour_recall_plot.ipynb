{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_dir = 'outputs/token_vs_contour_recall'\n",
    "\n",
    "\n",
    "tokenizers = [\n",
    "    'directsam_large_sa1b_2ep', \n",
    "    'directsam_large_gen1_1008', \n",
    "    'directsam_large_gen2_1014', \n",
    "    'directsam_large_gen3_1023', \n",
    "\n",
    "    'directsam_tiny_sa1b_2ep', \n",
    "    'directsam_tiny_dsa_50ep', \n",
    "    'directsam_tiny_dsa_75ep', \n",
    "\n",
    "    'panoptic_mask2former_base', \n",
    "    'panoptic_mask2former_large', \n",
    "    'panoptic_mask2former_small', \n",
    "    'panoptic_mask2former_tiny', \n",
    "    'panoptic_oneformer_large', \n",
    "    'panoptic_oneformer_tiny', \n",
    "\n",
    "    'patch_2_per_side_raster', \n",
    "    'patch_4_per_side_raster', \n",
    "    'patch_8_per_side_raster', \n",
    "    'patch_16_per_side_raster', \n",
    "\n",
    "    'sam_vit_b', \n",
    "    'sam_vit_l', \n",
    "    'sam_vit_h', \n",
    "    # 'sam_vit_h_48points', \n",
    "    # 'sam_vit_h_64points', \n",
    "    # 'sam_vit_h_64points_1layer', \n",
    "    # 'sam_vit_h_64points_2layer', \n",
    "\n",
    "    'superpixel_slic',\n",
    "    ]\n",
    "\n",
    "all_tokenizers = []\n",
    "summary = {}\n",
    "splits = os.listdir(output_dir)\n",
    "for split in splits:\n",
    "    split_dir = os.path.join(output_dir, split)\n",
    "    summary[split] = {}\n",
    "    files = os.listdir(split_dir)\n",
    "    files.sort()\n",
    "    for f in files:\n",
    "        if f.endswith('.json'):\n",
    "            results = json.load(open(os.path.join(split_dir, f)))\n",
    "            tokenizer = f.split('_202')[0]\n",
    "            all_tokenizers.append(tokenizer)\n",
    "            summary[split][tokenizer]= [results['mean_tokens'], results['mean_recall']]\n",
    "    \n",
    "    # print the difference between summary[split].keys() and tokenizers\n",
    "    missing = set(tokenizers) - set(summary[split].keys())\n",
    "    if missing:\n",
    "        print(f\"Missing tokenizers for {split}: {missing}\")\n",
    "\n",
    "print(summary)\n",
    "all_tokenizers = list(set(all_tokenizers))\n",
    "all_tokenizers.sort()\n",
    "for tokenizer in all_tokenizers:\n",
    "    print(tokenizer)\n",
    "    # check missing splits for this tokenizer\n",
    "    for split in splits:\n",
    "        if tokenizer not in summary[split]:\n",
    "            print(f\" - Missing {split} for {tokenizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_LEVELS = [\n",
    "    'tcd', 'PhenoBench', 'EgoHOS', 'LoveDA', \n",
    "    'SA1B', \n",
    "    'DUTS_TE', 'LVIS', \n",
    "    'COCONut_relabeld_COCO_val', \n",
    "    'FoodSeg103', 'plantorgans', 'SUIM', \n",
    "    'LIP', \n",
    "    'MyFood', 'DIS5K_DIS_VD', 'WireFrame', 'EntitySeg', 'Fashionpedia', 'SOBA', \n",
    "    'ISAID', 'MapillaryMetropolis', 'PACO', \n",
    "    'PascalPanopticParts', \n",
    "    'ADE20k', \n",
    "    'SeginW', \n",
    "    'CIHP', 'NYUDepthv2', 'cityscapes', 'DRAM', \n",
    "    'PartImageNetPP', \n",
    "    'SPIN'\n",
    "]\n",
    "\n",
    "OBJECT_LEVEL = [\n",
    "    \"EgoHOS\", \"DUTS_TE\", \"LVIS\",\n",
    "    \"FoodSeg103\", \"SUIM\", \"MyFood\", \"DIS5K_DIS_VD\", \"EntitySeg\", \"NYUDepthv2\", \"cityscapes\", \"DRAM\"\n",
    "]\n",
    "\n",
    "\n",
    "SUBOBJECT_LEVEL = [\n",
    "    \"LIP\", \"PACO\", \"PascalPanopticParts\", \"ADE20k\", \"CIHP\", \"PartImageNetPP\", \"SPIN\", \"Fashionpedia\"\n",
    "]\n",
    "\n",
    "classification = {\n",
    "    'All': ALL_LEVELS,\n",
    "    'Object': OBJECT_LEVEL,\n",
    "    'Subobject': SUBOBJECT_LEVEL,\n",
    "}\n",
    "\n",
    "for name, SPLITS in classification.items():\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for tokenizer in tokenizers:\n",
    "        avg_tokens = 0\n",
    "        avg_recall = 0\n",
    "        for split in SPLITS:\n",
    "            if tokenizer not in summary[split]:\n",
    "                raise ValueError(f\"Missing {tokenizer} for {split}\")\n",
    "            tokens, recall = summary[split][tokenizer]\n",
    "            avg_tokens += tokens\n",
    "            avg_recall += recall\n",
    "        avg_tokens /= len(SPLITS)\n",
    "        avg_recall /= len(SPLITS)\n",
    "\n",
    "        plt.scatter(avg_tokens, avg_recall, label=tokenizer)\n",
    "        plt.text(avg_tokens, avg_recall, f\"{tokenizer}\\n{int(avg_tokens)}:{avg_recall*100:.1f}\", fontsize=12)\n",
    "\n",
    "\n",
    "    plt.xlabel('Average Tokens')\n",
    "    plt.ylabel('Average Recall')\n",
    "    # plt.title(f'Token vs Contour Recall ({SPLITS})')\n",
    "    # instead use var name of SPLITS\n",
    "    plt.title(f'Token vs Contour Recall ({name})')\n",
    "    plt.xlim(0, 256)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
