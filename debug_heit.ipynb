{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "from datasets import load_dataset\n",
    "from utils.visualization import visualize_masks\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def load_samples(output_dir, split, resolution, model):\n",
    "    samples_dict = {}\n",
    "    results_dir = f'{output_dir}/{split}/{resolution}/{model}'\n",
    "    for file in os.listdir(results_dir):\n",
    "        samples_dict[int(file.split('.')[0])] = os.path.join(results_dir, file)\n",
    "\n",
    "    samples = []\n",
    "    for index in range(len(samples_dict)):\n",
    "        samples.append(samples_dict[index])\n",
    "\n",
    "    return samples\n",
    "\n",
    "def decode_masks(mask_rles):\n",
    "    masks = []\n",
    "    for mask_rle in mask_rles:\n",
    "        mask = mask_util.decode(mask_rle)\n",
    "        masks.append(mask)\n",
    "    masks = np.array(masks)\n",
    "    mask_sums = masks.sum(axis=(1, 2))\n",
    "    masks = masks[mask_sums > 0]\n",
    "    return masks\n",
    "\n",
    "\n",
    "def label_map_to_random_rgb(label_map, seed=None):\n",
    "    \"\"\"\n",
    "    Convert a (H, W) integer label map into a (H, W, 3) random RGB image.\n",
    "\n",
    "    Args:\n",
    "        label_map (torch.Tensor or np.ndarray): Single-channel label map of shape (H, W).\n",
    "        seed (int, optional): If provided, sets a random seed for reproducible colors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Color image of shape (H, W, 3) with dtype=np.uint8.\n",
    "    \"\"\"\n",
    "    # If user wants reproducible colors, set the seed\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Convert label_map to a CPU tensor (if torch) or ndarray\n",
    "    if isinstance(label_map, torch.Tensor):\n",
    "        label_map = label_map.cpu().numpy()\n",
    "    label_map = label_map.astype(np.int32)\n",
    "\n",
    "    # Determine the number of classes (maximum label)\n",
    "    max_label = label_map.max()\n",
    "    if max_label < 1:\n",
    "        # If there are no labels > 0, just return a blank image\n",
    "        h, w = label_map.shape\n",
    "        return np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    # Generate random colors for each label: shape (max_label+1, 3)\n",
    "    #   e.g., color 0 = black, color 1 = random, color 2 = random, etc.\n",
    "    #   If you want label 0 to also have a random color, just remove the special case for 0.\n",
    "    colors = np.zeros((max_label + 1, 3), dtype=np.uint8)\n",
    "    for lbl in range(1, max_label + 1):\n",
    "        colors[lbl] = np.random.randint(0, 256, size=3)\n",
    "\n",
    "    # Map each label to its corresponding color\n",
    "    rgb_image = colors[label_map]\n",
    "\n",
    "    return rgb_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masks_to_label_map(masks, device='cuda', output_size=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Converts multiple binary masks into a single labeled map (with optional resizing).\n",
    "\n",
    "    Args:\n",
    "        masks (torch.Tensor or array-like): A tensor (or array) of shape (N, H, W), \n",
    "            where N is the number of masks.\n",
    "        device (str): The device to place the tensor (e.g. 'cpu', 'cuda').\n",
    "        output_size (tuple): The desired output height and width (H_out, W_out).\n",
    "\n",
    "    Returns:\n",
    "        torch.LongTensor: A labeled map of shape (H_out, W_out).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure masks are in torch.Tensor format and move to device\n",
    "    if not isinstance(masks, torch.Tensor):\n",
    "        masks = torch.tensor(masks, device=device)\n",
    "    else:\n",
    "        masks = masks.to(device)\n",
    "\n",
    "    # Create label map\n",
    "    #   Each non-zero region in mask i is labeled with (i+1).\n",
    "    label_map = torch.zeros_like(masks[0], dtype=torch.int64)\n",
    "    for i, mask in enumerate(masks):\n",
    "        if torch.sum(mask) == 0:\n",
    "            continue\n",
    "        label_map += (i + 1) * mask\n",
    "\n",
    "    # Optionally resize label_map to (H_out, W_out)\n",
    "    if output_size is not None:\n",
    "        label_map = label_map.unsqueeze(0).unsqueeze(0).float()  # Shape: (1,1,H,W)\n",
    "        label_map = F.interpolate(label_map, size=output_size, mode='nearest')\n",
    "        label_map = label_map.squeeze(0).squeeze(0).long()       # Shape: (H_out, W_out)\n",
    "\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def label_map_to_contour(label_map, tolerance):\n",
    "    \"\"\"\n",
    "    Converts a labeled map into a contour (boundary) map using dilation/erosion.\n",
    "\n",
    "    Args:\n",
    "        label_map (torch.LongTensor): A labeled map of shape (H, W).\n",
    "        tolerance (int): The size of the neighborhood to consider for detecting edges.\n",
    "\n",
    "    Returns:\n",
    "        torch.BoolTensor: A boolean tensor of shape (H, W) indicating the contour.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare label_map for max_pool2d (batch and channel dimension)\n",
    "    label_map_4d = label_map.unsqueeze(0).unsqueeze(0).float()  # (1,1,H,W)\n",
    "\n",
    "    # Perform dilation\n",
    "    dilated = F.max_pool2d(\n",
    "        label_map_4d,\n",
    "        kernel_size=2 * tolerance + 1,\n",
    "        stride=1,\n",
    "        padding=tolerance\n",
    "    )\n",
    "    # Perform erosion (negate, max_pool, then negate back)\n",
    "    eroded = -F.max_pool2d(\n",
    "        -label_map_4d,\n",
    "        kernel_size=2 * tolerance + 1,\n",
    "        stride=1,\n",
    "        padding=tolerance\n",
    "    )\n",
    "\n",
    "    # Detect boundaries\n",
    "    boundaries = (dilated != eroded).squeeze(0).squeeze(0).bool()\n",
    "\n",
    "    # Exclude background (label=0) from boundaries\n",
    "    boundaries &= (label_map != 0)\n",
    "\n",
    "    return boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def contour_metrics(\n",
    "    contour_gt: torch.Tensor,\n",
    "    contour_pred: torch.Tensor,\n",
    "    contour_pred_dilated: torch.Tensor,\n",
    "    tolerance: int = 5,\n",
    "    eps: float = 1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1-score between two boolean boundary maps,\n",
    "    ignoring the outer 'tolerance' pixels on each edge.\n",
    "\n",
    "    Args:\n",
    "        contour_gt (torch.Tensor): Ground truth boundary map (bool), shape (H, W).\n",
    "        contour_pred (torch.Tensor): Predicted boundary map (bool), shape (H, W).\n",
    "        tolerance (int): Number of pixels to crop from each edge.\n",
    "        eps (float): A small value to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'precision', 'recall', and 'f1' as floats.\n",
    "    \"\"\"\n",
    "    # Ensure both tensors are boolean\n",
    "    contour_gt = contour_gt.bool()\n",
    "    contour_pred = contour_pred.bool()\n",
    "    contour_pred_dilated = contour_pred_dilated.bool()\n",
    "\n",
    "    # Verify that the image is large enough to crop\n",
    "    cropped_gt = contour_gt[tolerance:-tolerance, tolerance:-tolerance]\n",
    "    cropped_pred = contour_pred[tolerance:-tolerance, tolerance:-tolerance]\n",
    "    contour_pred_dilated = contour_pred_dilated[tolerance:-tolerance, tolerance:-tolerance]\n",
    "\n",
    "    # compute precision with raw prediction\n",
    "    tp = torch.sum(cropped_gt & cropped_pred).float()\n",
    "    fp = torch.sum(~cropped_gt & cropped_pred).float()\n",
    "    fn = torch.sum(cropped_gt & ~cropped_pred).float()\n",
    "    precision = tp / (tp + fp + eps)\n",
    "\n",
    "    # compute recall with dilated prediction\n",
    "    tp = torch.sum(cropped_gt & contour_pred_dilated).float()\n",
    "    fp = torch.sum(~cropped_gt & contour_pred_dilated).float()\n",
    "    fn = torch.sum(cropped_gt & ~contour_pred_dilated).float()\n",
    "    recall = tp / (tp + fn + eps)\n",
    "\n",
    "\n",
    "    # Compute F1\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "\n",
    "    return {\n",
    "        'precision': precision.item(),\n",
    "        'recall': recall.item(),\n",
    "        'f1': f1.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_dir = 'evaluation_intrinsic/outputs/segmentation_results'\n",
    "output_dir = 'evaluation_intrinsic/outputs/segmentation_metrics'\n",
    "\n",
    "resolutions = [\n",
    "    384, \n",
    "    768, \n",
    "    1024, \n",
    "    1500\n",
    "    ]\n",
    "\n",
    "splits = [\n",
    "    'SA1B', \n",
    "    'COCONut_relabeld_COCO_val', \n",
    "    'PascalPanopticParts', \n",
    "    'ADE20k',\n",
    "    'EgoHOS'\n",
    "    ]\n",
    "\n",
    "all_models = [\n",
    "    'directsam_large_sa1b_2ep@0.05', 'directsam_large_sa1b_2ep@0.1', 'directsam_large_sa1b_2ep@0.15', 'directsam_large_sa1b_2ep@0.2', \n",
    "    'directsam_large_sa1b_2ep@0.25', 'directsam_large_sa1b_2ep@0.3', 'directsam_large_sa1b_2ep@0.35', 'directsam_large_sa1b_2ep@0.4', \n",
    "    'directsam_large_sa1b_2ep@0.45', 'directsam_large_sa1b_2ep@0.5', 'directsam_tiny_sa1b_2ep@0.05', \n",
    "    'directsam_tiny_sa1b_2ep@0.1', 'directsam_tiny_sa1b_2ep@0.15', 'directsam_tiny_sa1b_2ep@0.2', 'directsam_tiny_sa1b_2ep@0.25', \n",
    "    'directsam_tiny_sa1b_2ep@0.3', 'directsam_tiny_sa1b_2ep@0.35', 'directsam_tiny_sa1b_2ep@0.4', 'directsam_tiny_sa1b_2ep@0.45', \n",
    "    'directsam_tiny_sa1b_2ep@0.5', \n",
    "    \n",
    "    'fastsam', 'mobilesamv2', \n",
    "    \n",
    "    'panoptic_mask2former_base', 'panoptic_mask2former_large', 'panoptic_mask2former_small', 'panoptic_mask2former_tiny', \n",
    "    'panoptic_mask2former_large_ade',\n",
    "    'panoptic_oneformer_large', 'panoptic_oneformer_tiny', \n",
    "    'panoptic_oneformer_large_coco',\n",
    "    \n",
    "    'patch_10_per_side_raster', 'patch_11_per_side_raster', 'patch_12_per_side_raster', 'patch_13_per_side_raster', 'patch_14_per_side_raster', 'patch_15_per_side_raster', \n",
    "    'patch_16_per_side_raster', 'patch_17_per_side_raster', 'patch_18_per_side_raster', 'patch_19_per_side_raster', 'patch_20_per_side_raster', 'patch_21_per_side_raster', \n",
    "    'patch_22_per_side_raster', 'patch_23_per_side_raster', 'patch_24_per_side_raster', 'patch_25_per_side_raster', 'patch_26_per_side_raster', 'patch_27_per_side_raster', \n",
    "    'patch_28_per_side_raster', 'patch_29_per_side_raster', 'patch_2_per_side_raster', 'patch_30_per_side_raster', 'patch_31_per_side_raster', 'patch_3_per_side_raster', \n",
    "    'patch_4_per_side_raster', 'patch_5_per_side_raster', 'patch_6_per_side_raster', 'patch_7_per_side_raster', 'patch_8_per_side_raster', 'patch_9_per_side_raster', \n",
    "    \n",
    "    'sam_vit_b', 'sam_vit_h', 'sam_vit_h_48points', 'sam_vit_h_64points', 'sam_vit_h_64points_1layer', 'sam_vit_l', \n",
    "    \n",
    "    'superpixel_slic_100', 'superpixel_slic_121', 'superpixel_slic_144', 'superpixel_slic_16', 'superpixel_slic_169', 'superpixel_slic_196', 'superpixel_slic_225', \n",
    "    'superpixel_slic_25', 'superpixel_slic_256', 'superpixel_slic_36', 'superpixel_slic_4', 'superpixel_slic_49', 'superpixel_slic_64', 'superpixel_slic_81', 'superpixel_slic_9'\n",
    "    ]\n",
    "\n",
    "\n",
    "for resolution in resolutions:\n",
    "    for split in splits:\n",
    "        print('\\n\\n')\n",
    "        print('='*64)\n",
    "        print(split)\n",
    "        print('-'*64)\n",
    "        dataset = load_dataset(\"chendelong/HEIT\", split=split)\n",
    "        os.makedirs(f'{output_dir}/{split}/{resolution}', exist_ok=True)\n",
    "\n",
    "        models = []\n",
    "        for model in os.listdir(f'{results_dir}/{split}/{resolution}'):\n",
    "            samples_pred = load_samples(results_dir, split, resolution, model)\n",
    "            if len(samples_pred) == len(dataset):\n",
    "                models.append(model)\n",
    "            else:\n",
    "                print(f'{model} at {resolution} resolution only has {len(samples_pred)}/{len(dataset)} samples')\n",
    "\n",
    "        # find and print missing models\n",
    "        missing_models = set(all_models) - set(models)\n",
    "        additional_models = set(models) - set(all_models)\n",
    "\n",
    "        if missing_models:\n",
    "            print(\"Missing models:\")\n",
    "            for model in sorted(missing_models):\n",
    "                print('-', model)\n",
    "\n",
    "        if additional_models:\n",
    "            print(\"Additional models:\")\n",
    "            for model in sorted(additional_models):\n",
    "                print('-', model)\n",
    "\n",
    "        # all_models.extend(models)\n",
    "        # all_models = list(set(all_models))\n",
    "\n",
    "        print(f'{len(models)} models at {resolution} resolution in {split} dataset')\n",
    "        models.sort()\n",
    "        # print(models)\n",
    "        # for model in models:\n",
    "        #     print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_monosemanticity(\n",
    "    contour_gt: torch.Tensor, \n",
    "    label_map_pred: torch.LongTensor, \n",
    "    tolerance: int = 5\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Perform morphological erosion by `tolerance` for each label, then count how many\n",
    "    contour_gt pixels fall within each eroded label region.\n",
    "\n",
    "    Args:\n",
    "        contour_gt (torch.Tensor): Boolean tensor of shape (H, W), where True indicates \n",
    "            contour pixels.\n",
    "        label_map_pred (torch.LongTensor): Label map of shape (H, W), where 0..K = different segments.\n",
    "        tolerance (int): The amount (in pixels) by which to shrink each label region.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: Overlap counts (one per label, in ascending label_id order).\n",
    "    \"\"\"\n",
    "    unique_labels = torch.unique(label_map_pred)\n",
    "\n",
    "    # 1) Create a stacked binary mask for each label: shape (K, H, W)\n",
    "    label_stack = torch.stack([(label_map_pred == lbl) for lbl in unique_labels], dim=0)\n",
    "\n",
    "    # 2) Morphologically erode each label in a single batch\n",
    "    #    Convert to float: True -> 1.0, False -> 0.0\n",
    "    label_stack_4d = label_stack.unsqueeze(1).float()  # (K,1,H,W)\n",
    "    #    Erosion = -max_pool2d(-mask)\n",
    "    eroded_4d = -F.max_pool2d(\n",
    "        -label_stack_4d, \n",
    "        kernel_size=2 * tolerance + 1,\n",
    "        stride=1,\n",
    "        padding=tolerance\n",
    "    )\n",
    "    eroded_stack = eroded_4d.squeeze(1).bool()  # (K,H,W)\n",
    "\n",
    "    # 3) Count overlap with contour_gt\n",
    "    #    Expand contour_gt to shape (K,H,W) for batch-wise AND\n",
    "    overlap_tensor = eroded_stack & contour_gt.unsqueeze(0)\n",
    "    overlap_counts = overlap_tensor.sum(dim=(1,2))  # (K,)\n",
    "\n",
    "    # Return as a list (aligned with unique_labels order)\n",
    "    return overlap_counts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "# model = 'patch_31_per_side_raster'\n",
    "model = 'directsam_tiny_sa1b_2ep@0.1'\n",
    "\n",
    "results_dir = 'evaluation_intrinsic/outputs/segmentation_results'\n",
    "split = 'EgoHOS'\n",
    "resolution = 768\n",
    "\n",
    "dataset = load_dataset(\"chendelong/HEIT\", split=split)\n",
    "\n",
    "samples_pred = load_samples(results_dir, split, resolution, model)\n",
    "\n",
    "tolerance_recall = 5\n",
    "tolerance_monosemanticity = 25\n",
    "\n",
    "device = 'cuda'\n",
    "do_visualization = True\n",
    "\n",
    "total_area = resolution**2\n",
    "\n",
    "all_metrics = []\n",
    "for i in tqdm.tqdm(range(len(samples_pred))):\n",
    "    i = random.randint(0, len(samples_pred))\n",
    "    sample = dataset[i]\n",
    "    image = np.array(sample['image'])\n",
    "\n",
    "    contour_gt = np.array(sample['contour'])\n",
    "    contour_gt = torch.tensor(contour_gt, device=device).bool()\n",
    "\n",
    "    sample_pred = json.load(open(samples_pred[i]))\n",
    "\n",
    "    masks_pred = decode_masks(sample_pred['rles'])\n",
    "    masks_pred = torch.tensor(masks_pred, device=device)\n",
    "\n",
    "    label_map_pred = masks_to_label_map(masks_pred, output_size=(1024, 1024))\n",
    "    contour_pred_thin = label_map_to_contour(label_map_pred, 1)\n",
    "    contour_pred_dilated = label_map_to_contour(label_map_pred, tolerance_recall)\n",
    "\n",
    "    metrics = contour_metrics(contour_gt, contour_pred_thin, contour_pred_dilated, tolerance=tolerance_recall)\n",
    "    metrics['time'] = sample_pred['time']\n",
    "    metrics['n_tokens'] = masks_pred.shape[0]\n",
    "\n",
    "    mask_areas = masks_pred.sum(dim=(1, 2)) / total_area\n",
    "    mask_areas = mask_areas.cpu().numpy().tolist()\n",
    "    metrics['mask_areas'] = mask_areas\n",
    "    \n",
    "    monosemanticity = compute_monosemanticity(contour_gt, label_map_pred, tolerance_recall*5)\n",
    "    metrics['monosemanticity'] = monosemanticity\n",
    "    \n",
    "    print(metrics)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "    if do_visualization:\n",
    "\n",
    "        contour_gt = contour_gt.cpu().numpy().astype(np.uint8)\n",
    "        dilation_size = 2\n",
    "        kernel = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, \n",
    "            (2*dilation_size + 1, 2*dilation_size + 1)\n",
    "        )\n",
    "        contour_gt_dilated = cv2.dilate(contour_gt, kernel)\n",
    "        contour_gt_dilated = contour_gt_dilated.astype(bool)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(40, 10))\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "\n",
    "        label_map_vis = label_map_to_random_rgb(label_map_pred.cpu().numpy())\n",
    "        label_map_vis[contour_pred_thin.cpu().numpy()>0] = [255, 255, 255]\n",
    "        plt.imshow(label_map_vis)\n",
    "\n",
    "        plt.title(f\"{(masks_pred.sum(dim=(1, 2)) > 0).sum().item()} tokens\")\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.imshow(contour_pred_thin.cpu().numpy(), cmap='Blues', alpha=0.8)\n",
    "        plt.imshow(contour_pred_dilated.cpu().numpy(), cmap='Blues', alpha=0.3)\n",
    "        plt.imshow(contour_gt_dilated, cmap='Reds', alpha=0.5)\n",
    "        plt.imshow(image, alpha=0.1)\n",
    "        plt.title(f\"Predicted: P={metrics['precision']:.2f}, R={metrics['recall']:.2f}, F1={metrics['f1']:.2f}\")\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.imshow(contour_pred_thin.cpu().numpy(), cmap='Blues')\n",
    "        plt.imshow(image, alpha=0.3)\n",
    "\n",
    "        unique_labels = torch.unique(label_map_pred)\n",
    "\n",
    "        visual_label_map = torch.zeros_like(label_map_pred)\n",
    "        for lbl_i, lbl_id in enumerate(unique_labels):\n",
    "            if monosemanticity[lbl_i] == 0:\n",
    "            # if monosemanticity[lbl_i] == 0:\n",
    "                visual_label_map[label_map_pred == lbl_id] = 0\n",
    "            else:\n",
    "                visual_label_map[label_map_pred == lbl_id] = 1\n",
    "\n",
    "        visual_label_map_np = visual_label_map.cpu().numpy()\n",
    "        color_map = np.array([\n",
    "            [50, 150,   150],   # monosemantic\n",
    "            [255,  50,   50],  # not monosemantic\n",
    "        ], dtype=np.uint8)\n",
    "        rgb_image = color_map[visual_label_map_np]\n",
    "\n",
    "        plt.imshow(contour_gt_dilated, cmap='Reds')\n",
    "\n",
    "        rgb_image[contour_pred_thin.cpu().numpy()>0] = [255, 255, 255]\n",
    "        # plt.imshow(contour_pred.cpu().numpy(), cmap='Blues', alpha=0.2)\n",
    "        plt.imshow(rgb_image, alpha=0.5)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    break\n",
    "\n",
    "# output_file = f'{output_dir}/{split}/{resolution}/{model}.json'\n",
    "# json.dump(all_metrics, open(output_file, 'w'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
