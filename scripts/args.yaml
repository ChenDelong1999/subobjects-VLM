cwd: /private/home/delong/workspace/subobjects-VLM
conda_env_name: subobjects
conda_path: /private/home/delong/miniconda3
training_args:
  epoch: 3
  batch_size: 1
  gradient_accumulation_steps: 32
  dataset: sharegpt4v
  dataset_root: /private/home/delong/workspace/data/ShareGPT4V
  split: share-captioner_coco_lcs_sam_1246k_1107.json
  llm: HuggingFaceTB/SmolLM2-1.7B-Instruct
  visual_embed_config: configs/visual_embedding/clip_convnext_all.json
  max_visual_tokens: 16
  visual_tokenizer_config: configs/visual_tokenizer/directsam/directsam_tiny_dsa_100ep@0.1.json
  trainer_config: configs/training/sharegpt4v_pt.yaml
  embedding_input_resolution: 384
  tokenizer_input_resolution: 384
  dataloader_num_workers: 8


  # max_visual_tokens: 81
  # visual_tokenizer_config: configs/visual_tokenizer/patch/patch_9_per_side_random.json
  # visual_tokenizer_config: configs/visual_tokenizer/superpixel/superpixel_slic.json
  # visual_tokenizer_config: configs/visual_tokenizer/directsam/directsam_tiny_dsa_100ep@0.1.json
  # visual_tokenizer_config: configs/visual_tokenizer/directsam/directsam_tiny_dsa_101ep@0.5.json