{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "\n",
    "metrics_root = 'outputs/segmentation_metrics_0102'\n",
    "\n",
    "splits = [\n",
    "    # \"SA1B\", \n",
    "    # \"COCONut_relabeld_COCO_val\", \n",
    "    # \"PascalPanopticParts\", \n",
    "    # \"ADE20k\"\n",
    "    \"EgoHOS\"\n",
    "]\n",
    "resolutions = [\n",
    "    # 384, \n",
    "    768, \n",
    "    # 1024, \n",
    "    # 1500\n",
    "    ]\n",
    "\n",
    "def load_metrics(metrics_root, split, resolution, exlude=[]):\n",
    "    metrics_path = os.path.join(metrics_root, split, str(resolution))\n",
    "    all_metrics = {}\n",
    "\n",
    "    for file in os.listdir(metrics_path):\n",
    "        if file.endswith('.json'):\n",
    "            if file in exlude:\n",
    "                continue\n",
    "            with open(os.path.join(metrics_path, file), 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "                all_metrics[\n",
    "                    file.replace('.json', '')\n",
    "                ] = metrics\n",
    "\n",
    "    return all_metrics \n",
    "\n",
    "\n",
    "def compute_average(keys, metrics):\n",
    "    average = {}\n",
    "    for key in keys:\n",
    "        average[key] = round(sum([m[key] for m in metrics]) / len(metrics), 4)\n",
    "    average['n_samples'] = len(metrics)\n",
    "\n",
    "    all_monosemanticity = []\n",
    "    for metric in metrics:\n",
    "        count_nonzero = np.count_nonzero(metric['monosemanticity'])\n",
    "        monosemanticity = 1 - (count_nonzero / metric['n_tokens'])\n",
    "        all_monosemanticity.append(monosemanticity)\n",
    "\n",
    "    average['monosemanticity'] = round(np.average(all_monosemanticity) * 100, 4)\n",
    "\n",
    "    return average\n",
    "\n",
    "split = splits[0]\n",
    "resolution = resolutions[0]\n",
    "all_metrics = load_metrics(metrics_root, split, resolution, exlude=['superpixel_slic_4.json'])\n",
    "# print(all_metrics.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_avg = {}\n",
    "\n",
    "avg_keys = ['precision', 'recall', 'f1', 'time', 'n_tokens']\n",
    "for tokenizer, metrics in all_metrics.items():\n",
    "    avg = compute_average(avg_keys, metrics)\n",
    "    all_metrics_avg[tokenizer] = avg\n",
    "\n",
    "# sort by tokenizer name\n",
    "all_metrics_avg = dict(sorted(all_metrics_avg.items()))\n",
    "print(f\"Split: {split}, Resolution: {resolution}\")\n",
    "print(f'Loaded {len(all_metrics_avg)} metrics')\n",
    "df = pd.DataFrame(all_metrics_avg).T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot P-R Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_custom_prefix(name):\n",
    "    if name.startswith(\"directsam_tiny\"):\n",
    "        return \"DirectSAM-tiny\"\n",
    "    elif name.startswith(\"directsam_large\"):\n",
    "        return \"DirectSAM-large\"\n",
    "    elif name.startswith(\"panoptic_mask2former\"):\n",
    "        return \"Mask2Former (panoptic)\"\n",
    "    elif name.startswith(\"panoptic_oneformer\"):\n",
    "        return \"OneFormer (panoptic)\"\n",
    "    elif name.startswith(\"superpixel_slic\"):\n",
    "        return \"Superpixel\"\n",
    "    elif name == \"fastsam\":\n",
    "        return \"FastSAM\"\n",
    "    elif name == \"mobilesamv2\":\n",
    "        return \"MobileSAMv2\"\n",
    "    elif name.startswith(\"sam_vit\"):\n",
    "        return \"SAM\"\n",
    "    elif name.startswith(\"patch_\"):\n",
    "        return \"Patch\"\n",
    "    else:\n",
    "        return \"others\"\n",
    "\n",
    "df['prefix'] = df.index.map(get_custom_prefix)\n",
    "\n",
    "\n",
    "prefixes = df['prefix'].unique()\n",
    "colors = sns.color_palette(\"hls\", len(prefixes))\n",
    "prefix_color_map = dict(zip(prefixes, colors))\n",
    "\n",
    "for x, y in [('precision', 'recall'), ('n_tokens', 'monosemanticity')]:\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        subset = df[df['prefix'] == prefix].copy()\n",
    "        \n",
    "        # Sort this subset by precision so lines connect in ascending order\n",
    "        if x == 'precision':\n",
    "            subset.sort_values(y, inplace=True)\n",
    "        else:\n",
    "            subset.sort_values(x, inplace=True)\n",
    "        \n",
    "        # Scatter plot for the points\n",
    "        plt.scatter(\n",
    "            subset[x],\n",
    "            subset[y],\n",
    "            color=prefix_color_map[prefix],\n",
    "            label=prefix\n",
    "        )\n",
    "        \n",
    "        # Connect points with a line\n",
    "        plt.plot(\n",
    "            subset[x],\n",
    "            subset[y],\n",
    "            color=prefix_color_map[prefix]\n",
    "        )\n",
    "        \n",
    "        # Label each point\n",
    "        for idx in subset.index:\n",
    "            _x = subset.loc[idx, x]\n",
    "            _y = subset.loc[idx, y]\n",
    "            plt.text(_x, _y, idx, fontsize=8, ha='left', va='bottom')\n",
    "\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "\n",
    "    plt.legend(title='Tokenizer Family')\n",
    "\n",
    "    # You can adjust x-limits and y-limits as desired\n",
    "    # plt.xlim(0, 0.2) \n",
    "    # plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the DataFrame index into a column for tokenizer\n",
    "df_reset = df.reset_index().rename(columns={'index': 'tokenizer'})\n",
    "\n",
    "metrics = [\"f1\", \"time\", \"n_tokens\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=df_reset,\n",
    "        x=\"tokenizer\",\n",
    "        y=metric,\n",
    "        hue=\"prefix\",          # color-group by prefix\n",
    "        palette=prefix_color_map\n",
    "    )\n",
    "    \n",
    "    plt.title(metric.capitalize())\n",
    "    plt.xlabel(\"Tokenizer\")\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    \n",
    "    # Rotate the x-axis labels by 90 degrees\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Legend\n",
    "    plt.legend(title=\"Tokenizer Family\")\n",
    "    \n",
    "    # Adjust layout and show\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) A helper function to categorize tokenizer names into families:\n",
    "def get_custom_prefix(name):\n",
    "    if name.startswith(\"directsam_tiny\"):\n",
    "        return \"DirectSAM-tiny\"\n",
    "    elif name.startswith(\"directsam_large\"):\n",
    "        return \"DirectSAM-large\"\n",
    "    elif name.startswith(\"panoptic_mask2former\"):\n",
    "        return \"Mask2Former (panoptic)\"\n",
    "    elif name.startswith(\"panoptic_oneformer\"):\n",
    "        return \"OneFormer (panoptic)\"\n",
    "    elif name.startswith(\"superpixel_slic\"):\n",
    "        return \"Superpixel\"\n",
    "    elif name == \"fastsam\":\n",
    "        return \"FastSAM\"\n",
    "    elif name == \"mobilesamv2\":\n",
    "        return \"MobileSAMv2\"\n",
    "    elif name.startswith(\"sam_vit\"):\n",
    "        return \"SAM\"\n",
    "    elif name.startswith(\"patch_\"):\n",
    "        return \"Patch\"\n",
    "    else:\n",
    "        return \"others\"\n",
    "\n",
    "# 2) Build a color map: prefix -> distinct color\n",
    "all_prefixes = {get_custom_prefix(tkn) for tkn in all_metrics.keys()}\n",
    "colors = sns.color_palette(\"hls\", len(all_prefixes))\n",
    "prefix_color_map = dict(zip(all_prefixes, colors))\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "max_tokens = 1024\n",
    "\n",
    "# 3) Plot each tokenizer line\n",
    "for tokenizer in all_metrics.keys():\n",
    "    prefix = get_custom_prefix(tokenizer)\n",
    "    color = prefix_color_map[prefix]\n",
    "\n",
    "    metrics = all_metrics[tokenizer]\n",
    "    \n",
    "    # Collect sorted (and padded/truncated) areas for each metric\n",
    "    sorted_areas_list = []\n",
    "    for metric in metrics:\n",
    "        areas = np.array(metric['mask_areas'])\n",
    "        \n",
    "        # Sort descending:\n",
    "        areas = np.sort(areas)[::-1]\n",
    "        \n",
    "        # Pad or truncate to length = max_tokens\n",
    "        if len(areas) < max_tokens:\n",
    "            areas_padded = np.pad(\n",
    "                areas, \n",
    "                (0, max_tokens - len(areas)), \n",
    "                'constant', \n",
    "                constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            areas_padded = areas[:max_tokens]\n",
    "        \n",
    "        sorted_areas_list.append(areas_padded)\n",
    "    \n",
    "    # Convert to array (#metrics x max_tokens), then multiply by 100 for percentage\n",
    "    sorted_areas_list = np.array(sorted_areas_list) * 100\n",
    "    \n",
    "    # Average across all metrics for this tokenizer\n",
    "    avg_areas = sorted_areas_list.mean(axis=0)\n",
    "    \n",
    "    # Plot, using prefix-derived color. Label the line with the tokenizer name\n",
    "    plt.plot(range(1, max_tokens+1), avg_areas, label=tokenizer, color=color)\n",
    "\n",
    "# 4) Format plot\n",
    "plt.yscale('log')\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:.5f}%\"))\n",
    "plt.ylim(0.00001, 100)\n",
    "\n",
    "plt.xlim(0, max_tokens)\n",
    "\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Average Mask Area (% ratio to whole image) - log scale')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
