{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "\n",
    "# root = '/private/home/delong/workspace/subobjects-VLM/runs/clevr_caption/SmolLM2-135M-Instruct-vae(384px)'\n",
    "# splits = ['train', 'val']\n",
    "\n",
    "# root = '/private/home/delong/workspace/subobjects-VLM/runs/clevr_caption/SmolLM2-135M-Instruct-dinov2_small(384px)'\n",
    "# splits = ['train', 'val']\n",
    "\n",
    "# root = '/private/home/delong/workspace/subobjects-VLM/runs/pixmo_cap/Llama-3_2-1B-dinov2_small(768px)'\n",
    "# splits = ['train', 'val']\n",
    "\n",
    "# root = '/private/home/delong/workspace/subobjects-VLM/runs/imagenet/SmolLM2-135M-Instruct-dinov2_small(768px)'\n",
    "# splits = ['train', 'val']\n",
    "\n",
    "root = '/private/home/delong/workspace/subobjects-VLM/runs/sharegpt4v/Llama-3_2-1B-dinov2_small(768px)'\n",
    "splits = ['share-captioner_coco_lcs_sam_1246k_1107.json', 'sharegpt4v_instruct_gpt4-vision_cap100k.json']\n",
    "\n",
    "excluding = []\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_families = os.listdir(root)\n",
    "all_results = {}\n",
    "for tokenizer_family in tokenizer_families:\n",
    "    all_results[tokenizer_family] = []\n",
    "    runs = os.listdir(os.path.join(root, tokenizer_family))\n",
    "    for run in runs: \n",
    "        path = os.path.join(root, tokenizer_family, run, 'runs')\n",
    "        if run in excluding or not os.path.exists(path):\n",
    "            continue\n",
    "        files = os.listdir(path)\n",
    "        for file in files:\n",
    "            if file.startswith('vlm_eval') and file.endswith('.json'):\n",
    "                with open(os.path.join(path, file), 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    all_results[tokenizer_family].append(data)\n",
    "print(len(all_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_results(results):\n",
    "\n",
    "    def sort_key(result):\n",
    "        visual_tokenizer_config = result['checkpoint_args']['visual_tokenizer_config']\n",
    "        if '/patch' in visual_tokenizer_config:\n",
    "            return f\"{int(visual_tokenizer_config.split('patch_')[-1].split('_')[0]):2}\"\n",
    "        else:\n",
    "            return visual_tokenizer_config + f\"{result['checkpoint_args']['max_visual_tokens']:3}\"\n",
    "\n",
    "    # Sort the results based on the visual_tokenizer_config\n",
    "    results = sorted(results, key=sort_key)\n",
    "    return results\n",
    "\n",
    "for tokenizer_family in all_results.keys():\n",
    "    all_results[tokenizer_family] = sort_results(all_results[tokenizer_family])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "\n",
    "    for tokenizer_family, results in all_results.items():\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if result['evaluation_args']['split'] != split:\n",
    "                continue\n",
    "\n",
    "            if not result['evaluation_finished']:\n",
    "                print(f\"Skipping {result['evaluation_args']} because it didn't finish\")\n",
    "                continue\n",
    "\n",
    "            print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "color_offset = 15\n",
    "\n",
    "for split in splits:\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    cmaps = ['Greys', 'Blues', 'Reds', 'Greens']\n",
    "\n",
    "    for tokenizer_family, results in all_results.items():\n",
    "        colors = plt.cm.get_cmap(cmaps.pop(0), len(results)+color_offset)\n",
    "\n",
    "        previous_x, previous_y = None, None\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            if result['evaluation_args']['split'] != split:\n",
    "                continue\n",
    "\n",
    "            visual_tokenizer_config = result['checkpoint_args']['visual_tokenizer_config']\n",
    "            tokenizer_name = visual_tokenizer_config.split('/')[-1].replace('.json', '')\n",
    "            tokenizer_name = f\"{tokenizer_name} ({result['checkpoint_args']['max_visual_tokens']})\"\n",
    "\n",
    "            plt.scatter(result['average_visual_tokens'], result['average_loss'], label=tokenizer_name, color=colors(i+color_offset), s=100)\n",
    "            plt.text(result['average_visual_tokens'], result['average_loss'], f'{tokenizer_name}', color=colors(i+color_offset))\n",
    "\n",
    "            if previous_x is not None:\n",
    "                plt.plot([previous_x, result['average_visual_tokens']], [previous_y, result['average_loss']], color=colors(i+color_offset))\n",
    "\n",
    "            previous_x, previous_y = result['average_visual_tokens'], result['average_loss']\n",
    "\n",
    "    plt.title(split)\n",
    "    # plt.legend(loc='lower right')\n",
    "    # make x axis log scale\n",
    "    # plt.xscale('log')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subobjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
